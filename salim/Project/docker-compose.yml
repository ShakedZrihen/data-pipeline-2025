services:
  # S3 Simulator (LocalStack)
  s3:
    container_name: s3-simulator
    image: gresau/localstack-persist:4
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3
      - DEBUG=1
      - S3_SKIP_SIGNATURE_VALIDATION=1
      - AWS_DEFAULT_REGION=us-east-1
    volumes:
      - "localstack-data:/persisted-data"
    networks:
      - pipeline-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # S3 Initialization Service
  s3-init:
    image: python:3.11-slim
    container_name: s3-initializer
    depends_on:
      s3:
        condition: service_healthy
    environment:
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_DEFAULT_REGION=us-east-1
    volumes:
      - "./init-s3.py:/init-s3.py"
    networks:
      - pipeline-network
    command: sh -c "pip install boto3 && python /init-s3.py"

  # RabbitMQ
  rabbitmq:
    container_name: rabbitmq-server
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      - pipeline-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  # PostgreSQL Database
  postgres:
    container_name: postgres-db
    image: postgres:15
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-8HeXmxYnvy5xu}
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - pipeline-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Crawler Service
  crawler:
    build: ./crawler
    container_name: crawler-service
    ports:
      - "8080:8080"
    depends_on:
      s3-init:
        condition: service_completed_successfully
    environment:
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_DEFAULT_REGION=us-east-1
      - S3_ENDPOINT=http://s3:4566
      - LAMBDA_PORT=8080
      - S3_BUCKET=test-bucket
    volumes:
      - "./crawler:/app"
      - "/tmp:/tmp"
    networks:
      - pipeline-network
    command: sh -c "Xvfb :99 -screen 0 1024x768x24 & python lambda_handler.py & python cleanup_s3.py & while true; do python run.py; sleep 3600; done"

  # Extractor Service
  extractor:
    build: ./extractor
    container_name: extractor-service
    depends_on:
      s3-init:
        condition: service_completed_successfully
      rabbitmq:
        condition: service_healthy
    environment:
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_DEFAULT_REGION=us-east-1
      - S3_ENDPOINT=http://s3:4566
      - RABBITMQ_HOST=rabbitmq 
      - RABBITMQ_PORT=5672
      - RABBITMQ_USERNAME=admin
      - RABBITMQ_PASSWORD=admin
    volumes:
      - "./extractor:/app"
      - "./extractor/extracted_files:/app/extracted_files"
    networks:
      - pipeline-network
    command: sh -c "while true; do python extractor.py --all; sleep 1800; done"

  # Enricher Service
  enricher:
    build: ./enricher
    container_name: enricher-service
    depends_on:
      rabbitmq:
        condition: service_healthy
      postgres: 
        condition: service_healthy
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - RABBITMQ_HOST=rabbitmq 
      - RABBITMQ_PORT=5672
      - RABBITMQ_USERNAME=admin
      - RABBITMQ_PASSWORD=admin
      - PRICEFULL_QUEUE=pricefull_queue
      - PROMOFULL_QUEUE=promofull_queue
      - DLQ_QUEUE=dead_letter_queue
      - BATCH_SIZE=10
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      - LOG_LEVEL=INFO
      - FAST_TEST_MODE=${FAST_TEST_MODE:-false}
      - FAST_TEST_ITEM_LIMIT=${FAST_TEST_ITEM_LIMIT:-50}
    volumes:
      - "./enricher:/app"
      - "./enricher/Stores:/app/Stores"
    networks: 
      - pipeline-network
    command: sh -c "python create_tables.py && python load_stores.py && python queue_consumer.py"

  # API Service (Node.js)
  api:
    build: ../api
    container_name: api-service
    ports:
      - "3001:3001"
    depends_on: 
      postgres:
        condition: service_healthy
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - RABBITMQ_HOST=rabbitmq 
      - RABBITMQ_PORT=5672
      - RABBITMQ_USERNAME=admin
      - RABBITMQ_PASSWORD=admin
      - INPUT_QUEUE=all_data_queue
      - DLQ_QUEUE=dead_letter_queue
      - BATCH_SIZE=10
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      - LOG_LEVEL=INFO
    volumes:
      - "../api:/app"
      - "/app/node_modules"
    networks: 
      - pipeline-network
    command: npm start

  # Frontend Service
  frontend:
    build: ./crawler/frontend
    container_name: s3-frontend
    ports:
      - "3000:3000"
    volumes:
      - "./crawler/frontend/src:/app/src"
      - "./crawler/frontend/public:/app/public"
    depends_on:
      - crawler
    networks:
      - pipeline-network

networks:
  pipeline-network:
    driver: bridge

volumes:
  localstack-data:
  rabbitmq-data:
  postgres-data:
